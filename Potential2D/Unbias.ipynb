{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed modules and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- python script for write correct input files for \"ves_md_linearexpansion\" plumed module --#\n",
    "from input_VES import *\n",
    "#-- useful python script for training the DeepTICA cvs --#\n",
    "from utils import *\n",
    "\n",
    "#-- to not visualize warnings --#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- SIMULATION PARAMETERS --#\n",
    "sim_parameters = {\n",
    "    'nstep':50000000,#1000000, #long enough is 50000000, 10000000 -> 50ns of simulation\n",
    "    'plumedseed':4525,\n",
    "    'friction':10,\n",
    "    'temp':0.5, #kbt units (0.85)\n",
    "    'initial_position':[-1,0],\n",
    "    #-- parameters to compute the fes --#\n",
    "    'blocks':2,\n",
    "    'bandwidth': 0.02,\n",
    "    'plot_max_fes' :16,\n",
    "}\n",
    "#--------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Potential \n",
    "Just have a look at the Muller potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- MULLER --#\n",
    "#-- prepare grid points\n",
    "y = np.linspace(-2,2,300)\n",
    "x = np.linspace(-2,2,300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = potential2D(X,Y)\n",
    "#-- set to 0 the lowest basin --#\n",
    "Z-=np.min(Z)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,8)) \n",
    "#-- color map initialization --#\n",
    "#bounds = np.arange(0, 16, 1.)\n",
    "bounds = np.arange(0, 30, 0.5)\n",
    "cmap = plt.cm.get_cmap('fessa',len(bounds))\n",
    "colors = list(cmap(np.arange(len(bounds))))\n",
    "cmap = mpl.colors.ListedColormap(colors[:-1], \"\")\n",
    "# set over-color to last color of list \n",
    "cmap.set_over(\"white\")\n",
    "c = plt.pcolormesh(X, Y, Z, cmap=cmap,shading='auto',alpha=1,zorder=-1,\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False)\n",
    ")\n",
    "ax.set_xlabel(r\"$p.x$ [L]\")\n",
    "ax.set_ylabel(r\"$p.y$ [L]\")\n",
    "ax.set_title(r'$U(x,y)$ [$K_b T$]')\n",
    "\n",
    "#-- highlight the starting point --#\n",
    "ax.text(-1.5,0.2,r'$Starting$ $Position$',color=\"red\")\n",
    "draw_circle = plt.Circle((-1.2,0), 0.05, color=\"red\")\n",
    "ax.add_artist(draw_circle)\n",
    "\n",
    "fig.colorbar(c, ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input files for plumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"unbias/\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(folder+\"plumed.dat\",\"w\") as file:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# using natural units for Toy Model \n",
    "UNITS NATURAL\n",
    "\n",
    "# compute position for the one particle  \n",
    "p: POSITION ATOM=1\n",
    "# adding external potential \n",
    "potential: CUSTOM ARG=p.x,p.y FUNC=\"\"\"+Potential2D(),\"\"\" PERIODIC=NO\n",
    "ene: BIASVALUE ARG=potential\n",
    "\n",
    "# Print \n",
    "# STRIDE=200 so that the printed time is in 1 ps\n",
    "PRINT FMT=%g STRIDE=200 FILE=COLVAR ARG=p.x,p.y,ene.bias\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=file)\n",
    "\n",
    "#-- write input files for ves module --#\n",
    "generate_input_file(name_file=folder+\"input\",nstep=sim_parameters[\"nstep\"],temp=sim_parameters[\"temp\"],\n",
    "                    friction=sim_parameters[\"friction\"],random_seed=sim_parameters[\"plumedseed\"],\n",
    "                    initial_position=sim_parameters[\"initial_position\"])\n",
    "write_coeff(\"0\",folder+\"input\")\n",
    "\n",
    "#-- move necessary files for ves module --#\n",
    "execute(\"mv pot_coeffs_input.data unbias\",folder=\".\")\n",
    "#-- run plumed --#\n",
    "execute(\"plumed ves_md_linearexpansion input\",folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbias Trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "x,y = load_dataframe(\"unbias/COLVAR\").filter(regex=\"^p.x\").values,load_dataframe(\"unbias/COLVAR\").filter(regex=\"^p.y\").values\n",
    "ax.scatter(x,y,color=\"black\",alpha=1,label=\"Trajectory\",s=10)\n",
    "\n",
    "#-- prepare grid points\n",
    "y = np.linspace(-2,2,300)\n",
    "x = np.linspace(-2,2,300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = potential2D(X,Y)\n",
    "#-- set to 0 the lowest basin --#\n",
    "Z-=np.min(Z)\n",
    "\n",
    "#bounds = np.arange(np.min(Z), np.max(Z), 5.)\n",
    "bounds = np.arange(0, 30, 0.5)\n",
    "cmap = plt.cm.get_cmap('fessa',len(bounds))\n",
    "colors = list(cmap(np.arange(len(bounds))))\n",
    "cmap = mpl.colors.ListedColormap(colors[:-1], \"\")\n",
    "# set over-color to last color of list \n",
    "cmap.set_over(\"white\")\n",
    "\n",
    "c = plt.pcolormesh(X, Y, Z, cmap=cmap,shading='auto',alpha=1,zorder=-1,\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False)\n",
    ")\n",
    "c = plt.contourf(X, Y, Z, bounds , cmap=cmap,shading='auto',alpha=1,zorder=-1, linewidth=10,\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"Energu Surface\"\n",
    ")\n",
    "fig.colorbar(c, ax=ax)\n",
    "c = plt.contour(X, Y, Z, bounds , cmap=\"jet\",shading='auto',alpha=1, linewidth=5, linestyles=\"dashed\")\n",
    "#-- if put label on isolines --#\n",
    "#c.clabel()\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$p.x$ [L]\")\n",
    "ax.set_ylabel(r\"$p.y$ [L]\")\n",
    "ax.set_title(r'$U(x,y)$ [$K_b T$]')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- estimation of Free Energy Surface --#\n",
    "s = load_dataframe(\"unbias/COLVAR\").filter(regex=\"^p\").to_numpy()\n",
    "logweight=np.zeros(s.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        kbt=sim_parameters[\"temp\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([\"F(x) estimate\",\"F(y) estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(x,y)$ [L]\")\n",
    "ax.set_ylabel(r\"FES [$K_b T$]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbias simulation DeepTICA Analysis and Export  \n",
    "  \n",
    "Before performing the training we have to choose the `lag_time`. How to do that?  \n",
    "\n",
    "#### How to choose $\\tau$ \n",
    "The choice of $\\tau$ is strictly related to the timescales of the slow modes we are interested in. In addition due to the condition (2), explained in `README.md` file, we do not want overlap between eigenvalues. This mean that choosing either a really small or really large value for $\\tau$ it is not a wise choice. Physically speaking choosing a really small value for $\\tau$ means that all the slow modes are *slow*, and it becomes difficult to distinguish them. While choosing a very large value for $\\tau$ means that all the slow modes are *fast* and their eigenvalues will vanish. In principle every value of $\\tau$ in this interval would be good.. but what does it mean *small* and *large*?  \n",
    "  \n",
    "One possibility is to look directly at the time auto-correlation function $C(\\tau)$ of the descriptors $d_j(\\vec x)$, and from it one can estimate their correlation length $\\xi$, also called *memory timescale*. When the simulated system is in equilibrium one can expect that the time auto-correlation function decays as an exponential $e^{-\\frac{\\tau}{\\xi}}$. In this case $\\xi$ can be estimated by fitting $C(\\tau)$. But I will use a more generale definition of $\\xi$, which will give us a better estimation: $\\xi = \\int d \\tau C(\\tau)$\n",
    "  \n",
    "### Time auto-correlation function of the descriptors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(\"unbias/COLVAR\")\n",
    "descriptors_names = data.filter(regex='^p').columns.values\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[desc].plot.hist(bins=50,alpha=1,ax=ax,legend=False,grid=True,histtype='step',linewidth=2,density=True)\n",
    "    data[desc].plot.hist(bins=50,alpha=0.5,ax=ax,legend=False,grid=True,color=\"grey\",density=True)\n",
    "    ax.set_xlabel(desc + \" [L]\")\n",
    "    ax.set_title(desc)\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data.plot.scatter(x=\"time\",y=desc,ax=ax,legend=False,grid=True)\n",
    "    ax.set_xlabel(\"time [ps]\")\n",
    "    ax.set_title(desc)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation  \n",
    "\n",
    "$C(k) = \\dfrac{\\sum_i^{N-k} (Y_i - \\bar Y)(Y_{i+k} - \\bar Y) }{\\sum_i^{N} (Y_i - \\bar Y)^2}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Unbias Autocorrelation --#\n",
    "# this implementation is the same as *autocorr* method in pandas\n",
    "# but with this I can choose if either normalize the result or not\n",
    "def my_autocorrelation_pandaslike(x,lag,normed=True):\n",
    "    \n",
    "    mean = np.average(x,weights=None)\n",
    "    variance = np.cov(x,aweights=None)\n",
    "    N = len(x)\n",
    "      \n",
    "    x_lag = x[lag::]\n",
    "    x_0 = x[:(N-lag)]\n",
    "\n",
    "    if normed:\n",
    "        autocorr =  np.sum( np.multiply( (x_0-mean), (x_lag-mean) ) ) / ( (N-lag) * variance ) \n",
    "    else:\n",
    "        autocorr = np.sum( np.multiply( x_0, x_lag )) / (N-lag)\n",
    "    \n",
    "    return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- My Autocorrelation --#\n",
    "# with this implementation I recall and use the method implemented in *mlcvs* python package\n",
    "# more details and benchmark in *Time_Lagged_Dataset* folder\n",
    "def my_autocorrelation(x,lag,weight=None,time=None,normed=True):\n",
    "   \n",
    "    N = len(x)\n",
    "    if weight is None:\n",
    "        weight = np.ones(N)\n",
    "    if time is None:\n",
    "        time = np.arange(0,N)\n",
    "\n",
    "    data = create_time_lagged_dataset(x, t = time, lag_time = lag, logweights = np.log(weight))\n",
    "    x_t,x_lag,w_t,w_lag = np.array(data[:][0]),np.array(data[:][1]),np.array(data[:][2]),np.array(data[:][3])\n",
    "    Nw = np.sum(w_t)\n",
    "    mean = np.average(x_t,weights=w_t)\n",
    "    variance = np.cov(x_t,aweights=w_t)\n",
    "\n",
    "    if normed:\n",
    "        autocorr = np.sum( np.multiply( np.multiply( (x_t-mean), (x_lag-mean) ), w_lag ) ) / (Nw*variance)\n",
    "    else:\n",
    "        autocorr = np.sum( np.multiply( np.multiply( (x_t), (x_lag) ), w_lag ) ) / Nw\n",
    "\n",
    "    return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(\"unbias/COLVAR\")\n",
    "descriptors_names = data.filter(regex='^p').columns.values\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(14,5))#,sharey=True)\n",
    "\n",
    "last=7000\n",
    "x = np.linspace(0,last+1,last)\n",
    "acorr = np.empty(last)\n",
    "timescale = np.empty(len(descriptors_names))\n",
    "k=0\n",
    "for desc in descriptors_names:\n",
    "    print(\"autocorrelation for \", desc)\n",
    "    for i in range(last):\n",
    "        acorr[i] = data[desc].autocorr(i)\n",
    "    axs[0].plot(x,acorr)\n",
    "    timescale[k] = integrate.trapz(acorr[:last],x[:last])\n",
    "    k+=1\n",
    "\n",
    "times = pd.DataFrame(descriptors_names,columns=[\"descriptors\"])\n",
    "times[\"timescale\"] = timescale\n",
    "times.plot(kind=\"bar\",x=\"descriptors\",y=\"timescale\",rot=35,ax=axs[1],fontsize=15,label=r\"$\\xi$\")\n",
    "\n",
    "axs[0].set_xlabel(r'$\\tau$ [time] ')\n",
    "axs[0].set_title(r'$C(\\tau)$')\n",
    "axs[1].set_title(r'$\\xi=\\int d\\tau C(\\tau)$ [time]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(timescale))\n",
    "print(np.min(timescale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep-TICA cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(\"unbias/COLVAR\")\n",
    "size = len(data)\n",
    "min_lag,max_lag = 1,5 \n",
    "n = 5 # how many lag times between min and max lag\n",
    "lags = np.linspace(min_lag,max_lag,n) #-- how many batches for the train and valid set of a single simulation\n",
    "print(lags)\n",
    "shuffle = False # if shuffle the data between batches\n",
    "#-- train_datasets and valid_datasets list, it will be filled with new data every iteration\n",
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "# torch seed \n",
    "torch.manual_seed(21)\n",
    "\n",
    "descriptors_names = data.filter(regex='^p').columns.values\n",
    "\n",
    "#-- TRAINING PARAMETERS --#\n",
    "n_output = 2\n",
    "n_input = len(descriptors_names)\n",
    "train_parameters = {\n",
    "              'descriptors': '^p', # can change during simulation\n",
    "              'nodes':[n_input,10,n_output],\n",
    "              'activ_type': 'tanh',#'relu','selu','tanh'\n",
    "              'lag_time':10, \n",
    "              'loss_type': 'sum', \n",
    "              'n_eig': n_output,\n",
    "              'trainsize':0.8, \n",
    "              'lrate':5e-4,\n",
    "              'l2_reg':1e-5,\n",
    "              'num_epochs':600,\n",
    "              'batchsize': -1, #---> è da fare sul train loder and valid loader\n",
    "              'es_patience':100,\n",
    "              'es_consecutive':True,\n",
    "              'standardize_outputs':True,\n",
    "              'standardize_inputs': True,\n",
    "              'log_every':50,\n",
    "              }\n",
    "#--------------------------------------#\n",
    "\n",
    "# how many data in single batch, batchsize\n",
    "n_train = int( size*train_parameters[\"trainsize\"] )\n",
    "n_valid = int( size*(1-train_parameters[\"trainsize\"])-int(10*max_lag) )\n",
    "print(\"training samples: \",n_train, \"\\t validation samples\", n_valid)\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = data['time'].values\n",
    "X = data[descriptors_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time lagged dataset with different lag times\n",
    "for lag in lags:\n",
    "    #random split\n",
    "    # TensorDataset (x_t,x_lag,w_t,w_lag)\n",
    "    dataset = create_time_lagged_dataset(X,t=t,lag_time=lag,interval=[0,n_train+n_valid])\n",
    "    train_data, valid_data = random_split(dataset,[n_train,n_valid])\n",
    "    train_datasets.append(train_data)\n",
    "    valid_datasets.append(valid_data)\n",
    "\n",
    "train_loader = FastTensorDataLoader(train_datasets, batch_size=n_train,shuffle=shuffle)\n",
    "valid_loader = FastTensorDataLoader(valid_datasets, batch_size=n_valid,shuffle=shuffle)\n",
    "\n",
    "#-- TRAIN --#\n",
    "# MODEL\n",
    "model = DeepTICA_CV(train_parameters['nodes'],activation=train_parameters['activ_type'],gaussian_random_initialization=True)\n",
    "model.to(device)\n",
    "# OPTIMIZER (Adam)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=train_parameters['lrate'], weight_decay=train_parameters['l2_reg'])\n",
    "# lrscheduler\n",
    "#model.set_LRScheduler(opt,min_lr=5e-5)\n",
    "model.set_optimizer(opt)\n",
    "if valid_loader is not None:\n",
    "    # EarlyStopping\n",
    "    model.set_earlystopping(patience=train_parameters['es_patience'],\n",
    "                            min_delta=0.0,consecutive=train_parameters['es_consecutive'], save_best_model=True, log=False) \n",
    "# TRAIN\n",
    "model.fit(train_loader=train_loader,valid_loader=valid_loader,\n",
    "    standardize_inputs=train_parameters['standardize_inputs'],\n",
    "    standardize_outputs=train_parameters['standardize_outputs'],\n",
    "    loss_type=train_parameters['loss_type'],\n",
    "    n_eig=train_parameters['n_eig'],\n",
    "    nepochs=train_parameters['num_epochs'],\n",
    "    info=False, log_every=train_parameters['log_every'])\n",
    "#-- move the model back to cpu for convenience --#\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepTICA Analysis and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- move the model back to cpu for convenience --# \n",
    "model.to('cpu')\n",
    "\n",
    "#-- export checkpoint (for loading the model back to python) and torchscript traced module --#\n",
    "save_folder = folder+\"deeptica/\"\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    print(\"already exists\")\n",
    "#-- move to cpu before saving results --#\n",
    "model.to(\"cpu\")\n",
    "model.export(save_folder)\n",
    "print(\"model saved\")\n",
    "\n",
    "#-- print some useful results --#\n",
    "print(\"timescales: \",model.tica.timescales(train_parameters[\"lag_time\"]).detach().cpu().numpy()) \n",
    "print(\"eigenvalues: \",model.tica.evals_.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing model  \n",
    "  \n",
    "It is possible to load an existing model  \n",
    "\n",
    "```py\n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "search_values=\"^p.\"\n",
    "X, t, logweight = data.filter(regex=search_values).values, data['time'].to_numpy(). data[\"opes.bias\"].to_numpy()\n",
    "\n",
    "logweight = ( logweight-max(logweight) ) / sim_parameters[\"temp\"]\n",
    "\n",
    "model = DeepTICA_CV(train_parameters[\"nodes\"],activation=train_parameters['activ_type'])\n",
    "model.load_checkpoint(folder+\"deeptica/model_checkpoint.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding to data the cvs values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "data.plot.hist(y=\"cv1\",bins=20,ax=axs[0],density=True,color=\"b\")\n",
    "data.plot.hist(y=\"cv2\",bins=20,ax=axs[1],density=True,color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the correlation (*Pearson* correlation ,which simply means normed correlation) of the Deep-TICA cvs with the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(16,4))\n",
    "for k,cv in enumerate([\"cv1\",\"cv2\"]):\n",
    "    cols = [cv]\n",
    "    cols.extend(data.filter(regex='^p.').columns)\n",
    "    corr = data[cols].corr(method='pearson')\n",
    "\n",
    "    corr[cv].drop(cv).plot(kind='bar', ax=axs[k], rot=35)\n",
    "    axs[k].set_title('Correlation with DeepTICA '+str(k+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FES estimate from cvs  \n",
    "Obviously from this first simulation it is not possible to found a cvs that from data are able to distinguish all the possible basins. I recall that our approach is a **data drive approach**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- estimation of Free Energy Surface --#\n",
    "s = data.filter(regex=\"^cv\").to_numpy()\n",
    "logweight=np.zeros(s.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        kbt=sim_parameters[\"temp\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([\"F(cv1) estimate\",\"F(cv2) estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(cv1,cv2)$\")\n",
    "ax.set_ylabel(\"FES [KbT]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolines  \n",
    "We are working with a two dimensional Potential, and we are using as descriptors the two cartesian coordinates. This allows us to plot the isolines of the cvs on the physical space $(x,y)$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cvs_isolines(model,limits=((-2,2),(-2,2)),scatter=X)\n",
    "lim = ((np.min(X[:,0]),np.max(X[:,0])),(np.min(X[:,1]),np.max(X[:,1])))\n",
    "plot_cvs_isolines(model,limits=lim,scatter=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "for k,ax in enumerate(axs):\n",
    "    data.plot.scatter(y=\"p.y\",x=\"p.x\",c=\"cv\"+str(k+1),cmap=\"fessa\",ax=ax)\n",
    "    ax.set_title('Deep-TICA '+str(k+1))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "tica = TICA_CV(n_features=X.shape[1])\n",
    "tica.to(device)\n",
    "#tica.tica.symmetrize = False\n",
    "t = data['time'].values\n",
    "X = data[descriptors_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "tica.fit(X, t, lag=min_lag)\n",
    "\n",
    "'''\n",
    "save_folder = folder+\"tica/\"\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    print(\"already exists\")\n",
    "#-- move to cpu before saving results --#\n",
    "'''\n",
    "tica.to(\"cpu\")\n",
    "feature_names = data.filter(regex=\"p.*\").columns.values\n",
    "#tica.export(save_folder) --> non si può fare export del modelli tica. Trovare un modo\n",
    "#print(\"model saved\")\n",
    "tica.set_params({\"feature_names\": feature_names})\n",
    "#-- print some useful results --#\n",
    "#print(\"timescales: \",model.tica.timescales(train_parameters[\"lag_time\"]).detach().cpu().numpy()) \n",
    "print(\"eigenvalues: \",tica.tica.evals_.detach().cpu().numpy())\n",
    "\n",
    "print(tica.plumed_input().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cv1_tica\"] = np.transpose(tica(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2_tica\"] = np.transpose(tica(torch.Tensor(X)).detach().cpu().numpy())[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "data.plot.hist(y=\"cv1_tica\",bins=20,ax=axs[0],density=True,color=\"b\")\n",
    "data.plot.hist(y=\"cv2_tica\",bins=20,ax=axs[1],density=True,color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = ((np.min(X[:,0]),np.max(X[:,0])),(np.min(X[:,1]),np.max(X[:,1])))\n",
    "plot_cvs_isolines(tica,limits=lim,scatter=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "for k,ax in enumerate(axs):\n",
    "    data.plot.scatter(y=\"p.y\",x=\"p.x\",c=\"cv\"+str(k+1)+\"_tica\",cmap=\"fessa\",ax=ax)\n",
    "    ax.set_title('TICA '+str(k+1))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "#-- estimation of Free Energy Surface --#\n",
    "s = data[[\"cv1_tica\",\"cv2_tica\"]].to_numpy()#data.filter(regex=\"tica\").to_numpy()\n",
    "logweight=np.zeros(s.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        kbt=sim_parameters[\"temp\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([\"F(cv1) estimate\",\"F(cv2) estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(tica cv1,tica cv2)$\")\n",
    "ax.set_ylabel(\"FES [KbT]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality  \n",
    "We said that the **ICs** must satisfy two conditions. The first one is that they are uncorrelated, which means that $\\int d \\vec x \\psi_1(\\vec x) \\psi_2(\\vec x) e^{-\\beta U(\\vec x)} = 0$.  \n",
    "But their scalar product on the data will lead to a slightly different result, in this case approximately $0$, but not perfectly $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boltzmann_product(model,model,X,j=0,k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results reflects the fact that we have enforce the symmetrization of $C(\\tau)$  \n",
    "then one can enforce the orthogonality on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- check scalar product --#\n",
    "#DeepTICA 1\n",
    "cv1 = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "#DeepTICA 2 orthogonal to DeepTICA 1\n",
    "new_cv2 = orthogonal_cv(model,X)\n",
    "prod = np.multiply(np.multiply(cv1,np.ones(X.shape[0])),new_cv2).mean()\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cv2_orthogonal\"] = new_cv2\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "data.plot.scatter(y=\"p.y\",x=\"p.x\",c=\"cv2_orthogonal\",cmap=\"fessa\",ax=ax)\n",
    "ax.set_title('Deep-TICA 2 orthogonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the next simulation input files using TICA rather than Deeptica\n",
    "### Plumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"unbias/bias1_tica/\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(folder+\"plumed.dat\",\"w\") as file:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# using natural units for Toy Model \n",
    "UNITS NATURAL\n",
    "\n",
    "# compute position for the one particle  \n",
    "p: POSITION ATOM=1\n",
    "# adding external potential \n",
    "potential: CUSTOM ARG=p.x,p.y FUNC=\"\"\"+Potential2D()+\"\"\" PERIODIC=NO\n",
    "ene: BIASVALUE ARG=potential\n",
    "\n",
    "# define cv\n",
    "tica_cv1_0\"\"\"+\n",
    "tica.plumed_input().splitlines()[0][8:]+\"\"\"\\ntica_cv2_0\"\"\"+tica.plumed_input().splitlines()[1][8:]\n",
    "+\"\"\"\n",
    "# bias \n",
    "opes: OPES_METAD ARG=tica_cv1_0 TEMP=\"\"\"+str(sim_parameters[\"temp\"])+\"\"\" PACE=500 FILE=KERNELS BIASFACTOR=1.2 RESTART=NO BARRIER=3 STATE_WFILE=RestartKernels STATE_WSTRIDE=500*10\n",
    " \n",
    "PRINT FMT=%g STRIDE=200 FILE=COLVAR ARG=tica_cv1_0,tica_cv2_0,p.x,p.y,ene.bias,opes.*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=file)\n",
    "\n",
    "#-- write input files for ves module --#\n",
    "generate_input_file(name_file=folder+\"input\",nstep=sim_parameters[\"nstep\"],temp=sim_parameters[\"temp\"],\n",
    "                    friction=sim_parameters[\"friction\"],random_seed=sim_parameters[\"plumedseed\"],\n",
    "                    initial_position=sim_parameters[\"initial_position\"])\n",
    "write_coeff(\"0\",folder+\"input\")\n",
    "\n",
    "#-- move necessary files for ves module --#\n",
    "execute(\"mv pot_coeffs_input.data \"+folder,folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('mlcvs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "34f06b9f9681e9a7231f3100f6e351611d3f9f188d680707206d70c247429f78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
