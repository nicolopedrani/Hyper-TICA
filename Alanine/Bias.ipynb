{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed modules and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- useful python script for training the DeepTICA cvs --#\n",
    "from utils import *\n",
    "\n",
    "#-- to not visualize warnings --#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb=0.008314\n",
    "#-- SIMULATION PARAMETERS --#\n",
    "sim_parameters = {\n",
    "    'temp':300, \n",
    "    'beta': 1./(300*kb),\n",
    "    'kbt': None,\n",
    "    #-- parameters to compute the fes --#\n",
    "    'blocks':2,\n",
    "    'bandwidth': 0.02,\n",
    "    'plot_max_fes' :70,\n",
    "}\n",
    "#--------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input files for plumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"unbias_A/bias1/\"\n",
    "\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(folder+\"plumed.dat\",\"w\") as file:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "ene: ENERGY\n",
    "\n",
    "# include descriptors\n",
    "INCLUDE FILE=plumed_descriptors.data\n",
    "# define cv\n",
    "deep: PYTORCH_MODEL FILE=../deeptica/model.ptc ARG=d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,d20,d21,d22,d23,d24,d25,d26,d27,d28,d29,d30,d31,d32,d33,d34,d35,d36,d37,d38,d39,d40,d41,d42,d43,d44,d45\n",
    "# bias \n",
    "opes: OPES_METAD ARG=deep.node-0,deep.node-1 TEMP=300 PACE=500 FILE=KERNELS BARRIER=40 STATE_WFILE=RestartKernels STATE_WSTRIDE=500*10\n",
    " \n",
    "PRINT FMT=%g STRIDE=500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=file)\n",
    "\n",
    "#-- run gromacs --#\n",
    "#execute(\"cp script/input.* script/plumed_descriptors.data script/run_gromacs.sh \"+folder,folder=\".\")\n",
    "#execute(\"./run_gromacs.sh\",folder=folder)\n",
    "\n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "descriptors_names = data.filter(regex='^d[^a-z]').columns.values\n",
    "data_complete = load_dataframe(\"angles/COLVAR\")\n",
    "X_complete = data_complete[descriptors_names].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,8))\n",
    "load_dataframe(folder+\"COLVAR\").plot.scatter(y=\"psi\",x=\"phi\",ax=ax)\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "c = ax.contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c.clabel()\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "descriptors_names = data.filter(regex='^p').columns.values\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[desc].plot.hist(bins=50,alpha=1,ax=ax,legend=False,grid=True,histtype='step',linewidth=2,density=True)\n",
    "    data[desc].plot.hist(bins=50,alpha=0.5,ax=ax,legend=False,grid=True,color=\"grey\",density=True)\n",
    "    ax.set_title(desc)\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[::10].plot.scatter(x=\"time\",y=desc,alpha=1,ax=ax,legend=False,grid=True,linewidth=2,marker=\"^\")\n",
    "    data[::10].plot.line(x=\"time\",y=desc,alpha=1,ax=ax,legend=False,grid=True,color=\"grey\")\n",
    "    ax.set_title(desc)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- estimation of Free Energy Surface --#\n",
    "s = load_dataframe(folder+\"COLVAR\").filter(regex=\"^p\").to_numpy()\n",
    "logweight = np.transpose( load_dataframe(folder+\"COLVAR\").filter(regex=\"^opes.bias$\").to_numpy() )[0]*sim_parameters[\"beta\"]\n",
    "logweight = logweight - np.max(logweight)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        temp=sim_parameters[\"temp\"],\n",
    "                                        kbt=sim_parameters[\"kbt\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([r\"$F(\\phi)$ estimate\",r\"$F(\\psi)$ estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(\\phi,\\psi)$\")\n",
    "ax.set_ylabel(\"FES [Kj/mol]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridspec_fes(s,logweight,sim_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "descriptors_names = data.filter(regex='^d[^a-z]').columns.values\n",
    "\n",
    "fig,axs = plt.subplots(5,9,figsize=(22,10),sharey=True)\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[desc].plot.hist(bins=50,alpha=1,ax=ax,legend=False,grid=True,histtype='step',linewidth=2)#,density=True)\n",
    "    data[desc].plot.hist(bins=50,alpha=0.5,ax=ax,legend=False,grid=True,color=\"grey\")#,density=True)\n",
    "    ax.set_title(desc)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#-- compile cpp, longest procedure --#\n",
    "execute(\"make\",folder=folder)\n",
    "#-- remove old files\n",
    "#execute(\"rm w_lag.txt w_t.txt x_lag.txt x_t.txt\",folder=folder, print_result=False)\n",
    "\n",
    "#-- write input file to create time lagged dataset --#\n",
    "path = \"COLVAR\"\n",
    "names = load_dataframe(folder+path).filter(regex=\"^d[^a-z]\").columns\n",
    "first_index = data.columns.get_loc(names[0])\n",
    "last_index = data.columns.get_loc(names[-1])\n",
    "weights_index = data.columns.get_loc(\"opes.bias\")\n",
    "skip = 4 #skip the first four rows after the header \n",
    "with open(folder+\"input.dat\",\"w\") as file:\n",
    "    print(\"\"\"colvar = \"\"\"+path+\"\"\"\n",
    "skip_rows = \"\"\"+str(skip)+\"\"\"\n",
    "beta = \"\"\"+str(sim_parameters[\"beta\"])+\"\"\"\n",
    "lag = 1\n",
    "first_index = \"\"\"+str(first_index)+\"\"\"\n",
    "last_index = \"\"\"+str(last_index)+\"\"\"\n",
    "if_weights = 1\n",
    "weights_index = \"\"\"+str(weights_index)+\"\"\"\n",
    "\"\"\",file=file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fast way to produce time autocorrelation function \n",
    "\n",
    "#-- fast way, one descriptor at once --#\n",
    "#fig,axs = plt.subplots(2,1,figsize=(14,10))#,sharey=True)\n",
    "\n",
    "#-- in ps --#\n",
    "#last=10000\n",
    "#x = np.linspace(0,last+1,last)\n",
    "#acorr = np.array( [my_autocorrelation_cpp_all(\"input.dat\",lag=i,path=folder,weight=True) for i in range(last)] )\n",
    "#timescale = np.empty(len(descriptors_names))\n",
    "\n",
    "#for k in range(len(descriptors_names)):\n",
    "#    corr = acorr.T[k]\n",
    "#    axs[0].plot(x,corr)\n",
    "#    timescale[k] = integrate.trapz(corr[:last],x[:last])\n",
    "\n",
    "#times = pd.DataFrame(descriptors_names,columns=[\"descriptors\"])\n",
    "#times[\"timescale\"] = timescale\n",
    "#times.plot(kind=\"bar\",x=\"descriptors\",y=\"timescale\",rot=35,ax=axs[1],fontsize=15,label=r\"$\\xi$\")\n",
    "\n",
    "#axs[0].set_xlabel(r'$\\tau$')\n",
    "#axs[0].set_title(r'$C(\\tau)$')\n",
    "#axs[1].set_title(r'$\\xi=\\int d\\tau C(\\tau)$')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#print(np.max(timescale))\n",
    "#print(np.min(timescale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- TRAINING PARAMETERS --#\n",
    "n_output = 2\n",
    "n_input = 45\n",
    "train_parameters = {\n",
    "              'descriptors': '^d[^a-z]',\n",
    "              'nodes':[n_input,30,30,n_output], \n",
    "              'activ_type': 'tanh',\n",
    "              'lag_time':10,\n",
    "              'loss_type': 'sum', \n",
    "              'n_eig': n_output,\n",
    "              'trainsize':0.7,\n",
    "              'lrate':1e-3,\n",
    "              'l2_reg':0.,\n",
    "              'num_epochs':1500,\n",
    "              'earlystop':True,\n",
    "              'es_patience':300,\n",
    "              'es_consecutive':True,#False,\n",
    "              'standardize_outputs':True,\n",
    "              'standardize_inputs': True,\n",
    "              'log_every':500,\n",
    "              #if reweight the timescale\n",
    "              \"reweighting\": True,\n",
    "              \"path_cpp\": folder,#None\n",
    "              }\n",
    "#--------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for which lag time the gap is maximum  \n",
    "In this case che computational cost could be high, so I use the `create_time_lagged_dataset_cpp` implementation.  \n",
    "`train_parameters[path_cpp]` is the path to the folder in which execute `create_time_lagged_dataset_cpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' already done\n",
    "#-- write input file to create time lagged dataset --#\n",
    "path = \"COLVAR\"\n",
    "names = load_dataframe(folder+path).filter(regex=\"^d[^a-z]\").columns\n",
    "first_index = data.columns.get_loc(names[0])\n",
    "last_index = data.columns.get_loc(names[-1])\n",
    "weights_index = data.columns.get_loc(\"opes.bias\")\n",
    "skip = 4 #skip the first four rows after the header \n",
    "with open(folder+\"input.dat\",\"w\") as file:\n",
    "    print(\"\"\"colvar = \"\"\"+path+\"\"\"\n",
    "skip_rows = \"\"\"+str(skip)+\"\"\"\n",
    "beta = \"\"\"+str(sim_parameters[\"beta\"])+\"\"\"\n",
    "lag = 1\n",
    "first_index = \"\"\"+str(first_index)+\"\"\"\n",
    "last_index = \"\"\"+str(last_index)+\"\"\"\n",
    "if_weights = 1\n",
    "weights_index = \"\"\"+str(weights_index)+\"\"\"\n",
    "\"\"\",file=file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# per questi lags e con 3 semi diversi: python -> 50 minuti; cpp -> 18 minuti\n",
    "lags = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,18,25]\n",
    "# per questi faccio solo cpp\n",
    "lags = np.linspace(1,20,50)\n",
    "time = []\n",
    "seeds = [21]#,151],500]\n",
    "for seed in seeds:\n",
    "    timescale = np.empty(len(lags))\n",
    "    for i,lag in enumerate(lags):\n",
    "        train_parameters[\"lag_time\"] = lag\n",
    "        #-- modify lag time in input file --#\n",
    "        torch.manual_seed(seed)\n",
    "        model,data,logweight,X = training(sim_parameters[\"beta\"],folder+\"COLVAR\",train_parameters)\n",
    "\n",
    "        #-- move the model back to cpu for convenience --# \n",
    "        model.to('cpu')\n",
    "        #-- save gap between eigenvalues\n",
    "        timescale[i] = model.tica.evals_.detach().cpu().numpy()[0]-model.tica.evals_.detach().cpu().numpy()[1]\n",
    "    time.append(timescale)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#-- plot results --#\n",
    "for j,el in enumerate(time):\n",
    "    plt.plot(lags,el,label=\"sample\"+str(j))\n",
    "plt.legend()\n",
    "print(time[0])\n",
    "print(lags)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepTICA Analysis and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(21)\n",
    "train_parameters[\"lag_time\"] = 10\n",
    "train_parameters[\"path_cpp\"] = None\n",
    "#-- modify lag time in input file --#\n",
    "model,data,logweight,X = training(sim_parameters[\"beta\"],folder+\"COLVAR\",train_parameters)\n",
    "\n",
    "#-- move the model back to cpu for convenience --# \n",
    "model.to('cpu')\n",
    "\n",
    "#-- export checkpoint (for loading the model back to python) and torchscript traced module --#\n",
    "save_folder = folder+\"deeptica/\"\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    print(\"already exists\")\n",
    "#-- move to cpu before saving results --#\n",
    "model.to(\"cpu\")\n",
    "model.export(save_folder)\n",
    "print(\"model saved\")\n",
    "\n",
    "#-- print some useful results --#\n",
    "print(\"timescales: \",model.tica.timescales(train_parameters[\"lag_time\"]).detach().cpu().numpy()) \n",
    "print(\"eigenvalues: \",model.tica.evals_.detach().cpu().numpy())\n",
    "print(\"gap: \", model.tica.evals_.detach().cpu().numpy()[0]-model.tica.evals_.detach().cpu().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding to data the cvs values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]\n",
    "data_complete[\"cv1\"] = np.transpose(model(torch.Tensor(X_complete)).detach().cpu().numpy())[0]\n",
    "data_complete[\"cv2\"] = np.transpose(model(torch.Tensor(X_complete)).detach().cpu().numpy())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "data.plot.hist(y=\"cv1\",bins=20,ax=axs[0],density=True,color=\"b\")\n",
    "data.plot.hist(y=\"cv2\",bins=20,ax=axs[1],density=True,color=\"r\")\n",
    "data_complete.plot.hist(y=\"cv1\",bins=20,ax=axs[0],density=True,color=\"g\")\n",
    "data_complete.plot.hist(y=\"cv2\",bins=20,ax=axs[1],density=True,color=\"y\")\n",
    "# scatter\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,4))\n",
    "data.plot.scatter(y=\"cv1\",x=\"time\",ax=axs[0],color=\"b\")\n",
    "data.plot.scatter(y=\"cv2\",x=\"time\",ax=axs[1],color=\"r\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the correlation (*Pearson* correlation ,which simply means normed correlation) of the Deep-TICA cvs with the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1,figsize=(16,12),sharex=True)\n",
    "for k,cv in enumerate([\"cv1\",\"cv2\"]):\n",
    "    cols = [cv]\n",
    "    cols.extend(data.filter(regex=train_parameters[\"descriptors\"]).columns)\n",
    "    corr = data[cols].corr(method='pearson')\n",
    "    corr[cv].drop(cv).plot(kind='bar', ax=axs[k], rot=35, color=\"b\",label=r\"$C(deep|desc)$\")\n",
    "    axs[k].set_title('Correlation with DeepTICA '+str(k+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig,ax = plt.subplots(1,1,figsize=(18,6))\n",
    "#times[\"timescale\"] = times[\"timescale\"]/np.max(times[\"timescale\"])\n",
    "#cols = [\"cv1\"]\n",
    "#cols.extend(data.filter(regex=train_parameters[\"descriptors\"]).columns)\n",
    "#corr = data[cols].corr(method='pearson')\n",
    "#times[\"corr\"] = np.abs(corr[\"cv1\"].to_numpy()[1:])\n",
    "#times.plot(kind=\"bar\",x=\"descriptors\",rot=35,ax=ax,fontsize=15,stacked=False)\n",
    "#ax.set_title(r'Correlation with DeepTICA 1 vs $\\xi$')\n",
    "#ax.legend([r\"$\\frac{\\xi}{max(\\xi)}$\",r\"$|C(deep|desc)|$\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FES estimate from new cvs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- estimation of Free Energy Surface --#\n",
    "s = data.filter(regex=\"^cv\").to_numpy()\n",
    "logweight = np.transpose( load_dataframe(folder+\"COLVAR\").filter(regex=\"^opes.bias$\").to_numpy() )[0]*sim_parameters[\"beta\"]\n",
    "logweight = logweight - np.max(logweight)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        temp=sim_parameters[\"temp\"],\n",
    "                                        kbt=sim_parameters[\"kbt\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([\"F(cv1) estimate\",\"F(cv2) estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(cv1,cv2)$\")\n",
    "ax.set_ylabel(\"FES [Kj/mol]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FES estimate from old cvs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "s = data.filter(regex=\"^deep.node-\").to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        temp=sim_parameters[\"temp\"],\n",
    "                                        kbt=sim_parameters[\"kbt\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([\"F(deep1) estimate\",\"F(deep2) estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(deep1,deep2)$\")\n",
    "ax.set_ylabel(\"FES [Kj/mol]\")\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolines  \n",
    "We can use the data obtained from **angles** folder, biasing both $\\psi$ and $\\phi$ angles, to plot the isolines of the new Cvs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "for k,ax in enumerate(axs):\n",
    "    data.plot.scatter(y=\"psi\",x=\"phi\",c=\"cv\"+str(k+1),cmap=\"Set1\",ax=ax)\n",
    "    ax.set_xlabel(r\"$\\phi$\")\n",
    "    ax.set_ylabel(r\"$\\psi$\")\n",
    "    ax.set_title('Deep-TICA '+str(k+1))\n",
    "plt.tight_layout()\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "for k,ax in enumerate(axs):\n",
    "    data_complete.plot.scatter(y=\"psi\",x=\"phi\",c=\"cv\"+str(k+1),cmap=\"Set1\",ax=ax)\n",
    "    ax.set_xlabel(r\"$\\phi$\")\n",
    "    ax.set_ylabel(r\"$\\psi$\")\n",
    "    ax.set_title('Deep-TICA '+str(k+1))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' symbolic regression, file: hall_of_fame_2022-05-04_174509.174.csv\n",
    "X = data_complete[[\"phi\",\"psi\"]]#.to_numpy()\n",
    "y = data_complete[\"cv1\"]#.to_numpy()\n",
    "from pysr import PySRRegressor\n",
    "model = PySRRegressor(\n",
    "    niterations=5,\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        \"tanh\",\n",
    "        # non c'è step function\n",
    "        #\"inv(x) = 1/x\",  # Custom operator (julia syntax)\n",
    "    ],\n",
    "    model_selection=\"best\",\n",
    "    loss=\"loss(x, y) = (x - y)^2\",  # Custom loss function (julia syntax)\n",
    ")\n",
    "model.fit(X, y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh(0.63686156 - sin(x0 - -0.44599095)) best result // best score x0 = phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality  \n",
    "We said that the **ICs** must satisfy two conditions. The first one is that they are uncorrelated, which means that $\\int d \\vec x \\psi_1(\\vec x) \\psi_2(\\vec x) e^{-\\beta U(\\vec x)} = 0$.  \n",
    "But their scalar product on the data will lead to a slightly different result, in this case approximately $0$, but not perfectly $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boltzmann_product(model,model,X,j=0,k=1,logweight=logweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results reflects the fact that we have enforce the symmetrization of $C(\\tau)$  \n",
    "then one can enforce the orthogonality on the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
