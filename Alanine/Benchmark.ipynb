{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed modules and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- necessary modules --#\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "import matplotlib as mpl\n",
    "from scipy import integrate\n",
    "\n",
    "#-- to computer fes --#\n",
    "from mlcvs.utils.fes import compute_fes\n",
    "\n",
    "#-- to run process from jupyter --#\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "# execute bash command in the given folder\n",
    "def execute(command, folder, background=False):\n",
    "    cmd = subprocess.run(command, cwd=folder, shell=True, capture_output = True, text=True, close_fds=background)\n",
    "    if cmd.returncode == 0:\n",
    "        print(f'Completed: {command}')\n",
    "    else:\n",
    "        print(cmd.stderr)\n",
    "\n",
    "#-- useful python script for training the DeepTICA cvs --#\n",
    "from utils import *\n",
    "\n",
    "#-- to not visualize warnings --#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb=0.008314\n",
    "#-- SIMULATION PARAMETERS --#\n",
    "sim_parameters = {\n",
    "    'temp':300, \n",
    "    'beta': 1./(300*kb),\n",
    "    'kbt': None,\n",
    "    #-- parameters to compute the fes --#\n",
    "    'blocks':2,\n",
    "    'bandwidth': 0.02,\n",
    "    'plot_max_fes' :70,\n",
    "}\n",
    "#--------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input files for plumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"angles/\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(folder+\"plumed.dat\",\"w\") as file:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "ene: ENERGY\n",
    "\n",
    "INCLUDE FILE=plumed_descriptors.data\n",
    "\n",
    "opes: OPES_METAD ARG=phi,psi TEMP=300 PACE=500 BIASFACTOR=inf SIGMA=0.15,0.15 FILE=KERNELS BARRIER=40 STATE_WFILE=RestartKernels STATE_WSTRIDE=500*10\n",
    "\n",
    "PRINT FMT=%g STRIDE=500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=file)\n",
    "\n",
    "#-- run gromacs --#\n",
    "#execute(\"cp script/input.* script/plumed_descriptors.data \"+folder,folder=\".\")\n",
    "#execute(\"./run_gromacs.sh\",folder=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "load_dataframe(folder+\"COLVAR\").plot.scatter(y=\"psi\",x=\"phi\",c=\"opes.bias\",cmap=\"viridis\",ax=ax)\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- estimation of Free Energy Surface --#\n",
    "s = load_dataframe(folder+\"COLVAR\").filter(regex=\"^p\").to_numpy()\n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "logweight=( data[\"opes.bias\"].to_numpy()-max(data[\"opes.bias\"].to_numpy()) )*sim_parameters[\"beta\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i in range(2):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        temp=sim_parameters[\"temp\"],\n",
    "                                        kbt=sim_parameters[\"kbt\"],\n",
    "                                        blocks=sim_parameters[\"blocks\"],\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax)\n",
    "ax.legend([r\"$F(\\phi)$ estimate\",r\"$F(\\psi)$ estimate\"])   \n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel(r\"$(\\phi,\\psi)$\")\n",
    "ax.set_ylabel(\"FES [Kj/mol]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compare different set up for rescaled time\n",
    "fig,axs1 = plt.subplots(1,3,figsize=(25,4))\n",
    "fig,axs2 = plt.subplots(1,3,figsize=(25,4))\n",
    "fig,axs3 = plt.subplots(1,3,figsize=(25,4))\n",
    "fig,axs4 = plt.subplots(1,3,figsize=(25,4))\n",
    "    \n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "X = data.filter(regex='^d[^a-z]').values\n",
    "logweight = data[\"opes.bias\"].to_numpy()*sim_parameters[\"beta\"]\n",
    "t = data['time'].values\n",
    "dt = t[1]-t[0]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "axs1[0].plot(t,np.exp(logweight))\n",
    "axs1[0].set_title(\"(w,t) w\")\n",
    "axs2[0].plot(t,tprime)\n",
    "axs2[0].set_title(\"(t',t) w\")\n",
    "axs3[0].plot(t[1:],tprime[1:]/t[1:])\n",
    "axs3[0].set_title(\"(t'/t, t) w\")\n",
    "logweight -= max(logweight)\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "axs1[1].plot(t,np.exp(logweight))\n",
    "axs1[1].set_title(\"(w,t) w-max()\")\n",
    "axs2[1].plot(t,tprime)\n",
    "axs2[1].set_title(\"(t',t) w-max()\")\n",
    "axs3[1].plot(t[1:],tprime[1:]/t[1:])\n",
    "axs3[1].set_title(\"(t'/t,t) w-max()\")\n",
    "\n",
    "logweight = data[\"opes.bias\"].to_numpy()-max(data[\"opes.bias\"].to_numpy())\n",
    "logweight /= np.abs(np.min(logweight))\n",
    "logweight *= sim_parameters[\"beta\"]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "axs1[2].plot(t,np.exp(logweight))\n",
    "axs1[2].set_title(\"(w,t) (w-max())/np.abs(min)\")\n",
    "axs2[2].plot(t,tprime)\n",
    "axs2[2].set_title(\"(t',t) (w-max())/np.abs(min)\")\n",
    "axs3[2].plot(t[1:],tprime[1:]/t[1:])\n",
    "axs3[2].set_title(\"(t'/t,t) (w-max())/np.abs(min)\")\n",
    "\n",
    "#-- trajectories in phi / psi --#\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "for i in range(3):\n",
    "    data.plot.scatter(y=\"psi\",x=\"phi\",ax=axs4[i])\n",
    "    c = axs4[i].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    "    )\n",
    "    c.clabel()\n",
    "    axs4[i].grid()\n",
    "    axs4[i].set_xlabel(r\"$\\phi$\")\n",
    "    axs4[i].set_ylabel(r\"$\\psi$\")\n",
    "plt.tight_layout()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fig,axs1 = plt.subplots(1,3,figsize=(20,4))\n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "t = data['time'].values\n",
    "X = data.filter(regex='^d[^a-z]').values\n",
    "logweight = data[\"opes.bias\"].to_numpy()*sim_parameters[\"beta\"]\n",
    "dt = t[1]-t[0]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "x = data[\"phi\"].values\n",
    "#-- without minus max --#\n",
    "autocorr = np.empty(0)\n",
    "N = 100\n",
    "lags = np.linspace(0,10,N)\n",
    "for lag in lags:\n",
    "    res = my_autocorrelation_python(x,lag=lag,weight=np.exp(logweight),time=t, tprime=tprime)\n",
    "    autocorr = np.append(autocorr,res)\n",
    "axs1[0].plot(lags,autocorr)\n",
    "axs1[0].set_title(\"(t,w)\")\n",
    "#-- with minus max --#\n",
    "autocorr = np.empty(0)\n",
    "logweight -= max(logweight)\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "for lag in lags:\n",
    "    res = my_autocorrelation_python(x,lag=lag,weight=np.exp(logweight),time=t, tprime=tprime)\n",
    "    autocorr = np.append(autocorr,res)\n",
    "axs1[1].plot(lags,autocorr)\n",
    "axs1[1].set_title(\"(t,w) w-max() \")\n",
    "#-- with minus max and / np.abs(min)--#\n",
    "logweight = data[\"opes.bias\"].to_numpy()-max(data[\"opes.bias\"].to_numpy())\n",
    "logweight /= np.abs(np.min(logweight))\n",
    "autocorr = np.empty(0)\n",
    "logweight *= sim_parameters[\"beta\"]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))\n",
    "for lag in lags:\n",
    "    res = my_autocorrelation_python(x,lag=lag,weight=np.exp(logweight),time=t, tprime=tprime)\n",
    "    autocorr = np.append(autocorr,res)\n",
    "axs1[2].plot(lags,autocorr)\n",
    "axs1[2].set_title(\"(t,w) (w-max())/np.abs(min)\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 2d fes estimation\n",
    "s = load_dataframe(folder+\"COLVAR\").filter(regex=\"^p\").to_numpy()\n",
    "logweight= np.transpose( load_dataframe(folder+\"COLVAR\").filter(regex=\"^opes.bias$\").to_numpy() )[0]*sim_parameters[\"beta\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "gs = GridSpec(4, 4)#, figure=fig)\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[1:4,0:3])\n",
    "ax_hist_x = fig.add_subplot(gs[0,:3])\n",
    "ax_hist_y = fig.add_subplot(gs[1:,3])\n",
    "\n",
    "#-- 2D plot --#\n",
    "fes,grid,bounds,error = compute_fes(s, weights=np.exp(logweight),\n",
    "                                    temp=sim_parameters[\"temp\"],\n",
    "                                    kbt=sim_parameters[\"kbt\"],\n",
    "                                    blocks=sim_parameters[\"blocks\"],\n",
    "                                    bandwidth=sim_parameters[\"bandwidth\"],scale_by='range')\n",
    "                                    #,plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax_scatter)\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "cmap = plt.cm.get_cmap('fessa',len(bounds))\n",
    "colors = list(cmap(np.arange(len(bounds))))\n",
    "cmap = mpl.colors.ListedColormap(colors[:-1], \"\")\n",
    "# set over-color to last color of list \n",
    "cmap.set_over(\"white\")\n",
    "c = ax_scatter.pcolormesh(grid[0], grid[1], fes, cmap=cmap,shading='auto',alpha=1,\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\"\n",
    ")\n",
    "c = ax_scatter.contourf(grid[0], grid[1], fes, bounds , cmap=cmap,shading='auto',alpha=1, linewidth=10,\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "#fig.colorbar(c, ax=ax_scatter,label=\"FES [KbT]\")\n",
    "c = ax_scatter.contour(grid[0], grid[1], fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "ax_scatter.legend([\"FES [Kj/mol]\"])\n",
    "c.clabel()\n",
    "ax_scatter.grid()\n",
    "ax_scatter.set_xlabel(r\"$\\phi$\")\n",
    "ax_scatter.set_ylabel(r\"$\\psi$\")\n",
    "np.savetxt(folder+\"fes.txt\",fes,delimiter=\" \")\n",
    "np.savetxt(folder+\"grid0.txt\",grid[0],delimiter=\" \")\n",
    "np.savetxt(folder+\"grid1.txt\",grid[1],delimiter=\" \")\n",
    "\n",
    "#-- 1D plot --#\n",
    "fes,grid,bounds,error = compute_fes(s[:,0], weights=np.exp(logweight),\n",
    "                                    temp=sim_parameters[\"temp\"],\n",
    "                                    kbt=sim_parameters[\"kbt\"],\n",
    "                                    blocks=sim_parameters[\"blocks\"],\n",
    "                                    bandwidth=sim_parameters[\"bandwidth\"],scale_by='range')\n",
    "ax_hist_x.errorbar(grid,fes,yerr=error)\n",
    "ax_hist_x.set_ylabel(\"FES [Kj/mol]\")\n",
    "ax_hist_x.grid()\n",
    "                                    #,plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax_hist_y)\n",
    "fes,grid,bounds,error = compute_fes(s[:,1], weights=np.exp(logweight),\n",
    "                                    temp=sim_parameters[\"temp\"],\n",
    "                                    kbt=sim_parameters[\"kbt\"],\n",
    "                                    blocks=sim_parameters[\"blocks\"],\n",
    "                                    bandwidth=sim_parameters[\"bandwidth\"],scale_by='range')\n",
    "                                    #,plot=True, plot_max_fes=sim_parameters[\"plot_max_fes\"], ax = ax_hist_x)\n",
    "ax_hist_y.errorbar(fes,grid,xerr=error)\n",
    "ax_hist_y.set_xlabel(\"FES [Kj/mol]\")\n",
    "ax_hist_y.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "descriptors_names = data.filter(regex='^p').columns.values\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,4),sharey=True)\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[desc].plot.hist(bins=50,alpha=1,ax=ax,legend=False,grid=True,histtype='step',linewidth=2,density=True)\n",
    "    data[desc].plot.hist(bins=50,alpha=0.5,ax=ax,legend=False,grid=True,color=\"grey\",density=True)\n",
    "    ax.set_title(desc)\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "for ax,desc in zip(axs.flatten(),descriptors_names):\n",
    "    data[::100].plot.scatter(x=\"time\",y=desc,alpha=1,ax=ax,legend=False,grid=True,linewidth=2,marker=\"^\")\n",
    "    data[::100].plot.line(x=\"time\",y=desc,alpha=1,ax=ax,legend=False,grid=True,color=\"grey\")\n",
    "    ax.set_title(desc)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep TICA Analysis  \n",
    "Qui analizzo con la Deeptica la simulazione biased.  \n",
    "Lo farò utilizzando 4 diversi tprime per il tempo riscalato: \n",
    "1. senza usare il tempo riscalato \n",
    "2. usando il vero e proprio tempo riscalato con tprime = dt * exp(beta*logweights)\n",
    "3. come farebbe michele con tprime = dt * exp(beta*logweights-max(beta*logweights))\n",
    "4. come farei io con logweights -= max(logweights); logweights /= np.abs(np.min(logweights)); logweights \\*= beta, tprime = dt * exp(logweights)\n",
    "\n",
    "Results: \n",
    "Per tempi molti grandi nessuno degli algoritmi lavora bene. Per tempi piccoli, sulla scala del tempo di stride, lavora bene *1* e *4*, su tempi molto piccoli nessuno lavora bene.  \n",
    "Facendo a batches tra 1, 5 con 5 valori:   \n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIDE = 500 # stride of the simulation, usually 100 which means 1/5 ps\n",
    "dt = 0.000002 # time step of simulation, in nanoseconds\n",
    "time = 50 # in nanoseconds, time of single simulation\n",
    "size = (time/dt)/STRIDE # total sampled points for each simulation\n",
    "min_lag,max_lag = 1,5 #if stride is 100, 0.2,5 should be ok\n",
    "n = 5 # how many lag times between min and max lag\n",
    "lags = np.linspace(min_lag,max_lag,n) #-- how many batches for the train and valid set of a single simulation\n",
    "print(lags)\n",
    "shuffle = False # if shuffle the data between batches\n",
    "#-- train_datasets and valid_datasets list, it will be filled with new data every iteration\n",
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "# torch seed \n",
    "torch.manual_seed(21)\n",
    "\n",
    "# load data\n",
    "data = load_dataframe(folder+\"COLVAR\")\n",
    "descriptors_names = data.filter(regex='^d[^a-z]').columns.values\n",
    "\n",
    "#-- TRAINING PARAMETERS --#\n",
    "n_output = 2 # 2 non linear combination of the descriptors  \n",
    "n_input = len(descriptors_names) # can change..\n",
    "train_parameters = {\n",
    "              'descriptors': '^d[^a-z]', # can change during simulation\n",
    "              'nodes':[n_input,30,30,n_output],\n",
    "              'activ_type': 'tanh',#'relu','selu','tanh'\n",
    "              'lag_time':10, \n",
    "              'loss_type': 'sum', \n",
    "              'n_eig': n_output,\n",
    "              'trainsize':0.7, \n",
    "              'lrate':1e-3,\n",
    "              'l2_reg':0.,\n",
    "              'num_epochs':500,\n",
    "              'batchsize': -1, #---> è da fare sul train loder and valid loader\n",
    "              'es_patience':10,\n",
    "              'es_consecutive':True,\n",
    "              'standardize_outputs':True,\n",
    "              'standardize_inputs': True,\n",
    "              'log_every':50,\n",
    "              }\n",
    "\n",
    "# how many data in single batch, batchsize\n",
    "n_train = int( size*train_parameters[\"trainsize\"] )\n",
    "n_valid = int( size*(1-train_parameters[\"trainsize\"])-int(10*max_lag) )\n",
    "print(\"training samples: \",n_train, \"\\t validation samples\", n_valid)\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = data['time'].values\n",
    "X = data[descriptors_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time lagged dataset with different lag times\n",
    "for lag in lags:\n",
    "    #random split\n",
    "    # TensorDataset (x_t,x_lag,w_t,w_lag)\n",
    "    dataset = create_time_lagged_dataset(X,t=t,lag_time=np.round(lag,3),interval=[0,n_train+n_valid])\n",
    "    train_data, valid_data = random_split(dataset,[n_train,n_valid])\n",
    "    train_datasets.append(train_data)\n",
    "    valid_datasets.append(valid_data)\n",
    "\n",
    "train_loader = FastTensorDataLoader(train_datasets, batch_size=n_train,shuffle=shuffle)\n",
    "valid_loader = FastTensorDataLoader(valid_datasets, batch_size=n_valid,shuffle=shuffle)\n",
    "\n",
    "#-- TRAIN --#\n",
    "# MODEL\n",
    "model = DeepTICA_CV(train_parameters['nodes'],activation=train_parameters['activ_type'],gaussian_random_initialization=True)\n",
    "model.to(device)\n",
    "# OPTIMIZER (Adam)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=train_parameters['lrate'], weight_decay=train_parameters['l2_reg'])\n",
    "# lrscheduler\n",
    "#model.set_LRScheduler(opt,min_lr=5e-5)\n",
    "model.set_optimizer(opt)\n",
    "if valid_loader is not None:\n",
    "    # EarlyStopping\n",
    "    model.set_earlystopping(patience=train_parameters['es_patience'],\n",
    "                            min_delta=0.005,consecutive=train_parameters['es_consecutive'], save_best_model=True, log=False) \n",
    "# TRAIN\n",
    "model.fit(train_loader=train_loader,valid_loader=valid_loader,\n",
    "    standardize_inputs=train_parameters['standardize_inputs'],\n",
    "    standardize_outputs=train_parameters['standardize_outputs'],\n",
    "    loss_type=train_parameters['loss_type'],\n",
    "    n_eig=train_parameters['n_eig'],\n",
    "    nepochs=train_parameters['num_epochs'],\n",
    "    info=False, log_every=train_parameters['log_every'])\n",
    "#-- move the model back to cpu for convenience --#\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cvs #\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6),sharey=True)\n",
    "\n",
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]   \n",
    "\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv1\",cmap=\"Set1\",ax=ax[0])\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv2\",cmap=\"Set1\",ax=ax[1])\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "c = ax[0].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c = ax[1].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c.clabel()\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(r\"$\\phi$\")\n",
    "ax[0].set_ylabel(r\"$\\psi$\")\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(r\"$\\phi$\")\n",
    "ax[1].set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tprime\n",
    "logweight = data[\"opes.bias\"].to_numpy()*sim_parameters[\"beta\"]\n",
    "dt = t[1]-t[0]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "# create time lagged dataset with different lag times\n",
    "for lag in lags:\n",
    "    #random split\n",
    "    # TensorDataset (x_t,x_lag,w_t,w_lag)\n",
    "    dataset = create_time_lagged_dataset(X,t=t,lag_time=np.round(lag,3),tprime=tprime,logweights=logweight,interval=[0,n_train+n_valid])\n",
    "    train_data, valid_data = random_split(dataset,[n_train,n_valid])\n",
    "    train_datasets.append(train_data)\n",
    "    valid_datasets.append(valid_data)\n",
    "\n",
    "train_loader = FastTensorDataLoader(train_datasets, batch_size=n_train,shuffle=shuffle)\n",
    "valid_loader = FastTensorDataLoader(valid_datasets, batch_size=n_valid,shuffle=shuffle)\n",
    "\n",
    "#-- TRAIN --#\n",
    "# MODEL\n",
    "model = DeepTICA_CV(train_parameters['nodes'],activation=train_parameters['activ_type'],gaussian_random_initialization=True)\n",
    "model.to(device)\n",
    "# OPTIMIZER (Adam)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=train_parameters['lrate'], weight_decay=train_parameters['l2_reg'])\n",
    "# lrscheduler\n",
    "#model.set_LRScheduler(opt,min_lr=5e-5)\n",
    "model.set_optimizer(opt)\n",
    "if valid_loader is not None:\n",
    "    # EarlyStopping\n",
    "    model.set_earlystopping(patience=train_parameters['es_patience'],\n",
    "                            min_delta=0.005,consecutive=train_parameters['es_consecutive'], save_best_model=True, log=False) \n",
    "# TRAIN\n",
    "model.fit(train_loader=train_loader,valid_loader=valid_loader,\n",
    "    standardize_inputs=train_parameters['standardize_inputs'],\n",
    "    standardize_outputs=train_parameters['standardize_outputs'],\n",
    "    loss_type=train_parameters['loss_type'],\n",
    "    n_eig=train_parameters['n_eig'],\n",
    "    nepochs=train_parameters['num_epochs'],\n",
    "    info=False, log_every=train_parameters['log_every'])\n",
    "#-- move the model back to cpu for convenience --#\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cvs #\n",
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]   \n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv1\",cmap=\"Set1\",ax=ax[0])\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv2\",cmap=\"Set1\",ax=ax[1])\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "c = ax[0].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c = ax[1].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c.clabel()\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(r\"$\\phi$\")\n",
    "ax[0].set_ylabel(r\"$\\psi$\")\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(r\"$\\phi$\")\n",
    "ax[1].set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tprime\n",
    "logweight = data[\"opes.bias\"].to_numpy()*sim_parameters[\"beta\"]-max(data[\"opes.bias\"].to_numpy()*sim_parameters[\"beta\"])\n",
    "dt = t[1]-t[0]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "# create time lagged dataset with different lag times\n",
    "for lag in lags:\n",
    "    #random split\n",
    "    # TensorDataset (x_t,x_lag,w_t,w_lag)\n",
    "    dataset = create_time_lagged_dataset(X,t=t,lag_time=np.round(lag,3),tprime=tprime,logweights=logweight,interval=[0,n_train+n_valid])\n",
    "    train_data, valid_data = random_split(dataset,[n_train,n_valid])\n",
    "    train_datasets.append(train_data)\n",
    "    valid_datasets.append(valid_data)\n",
    "\n",
    "train_loader = FastTensorDataLoader(train_datasets, batch_size=n_train,shuffle=shuffle)\n",
    "valid_loader = FastTensorDataLoader(valid_datasets, batch_size=n_valid,shuffle=shuffle)\n",
    "\n",
    "#-- TRAIN --#\n",
    "# MODEL\n",
    "model = DeepTICA_CV(train_parameters['nodes'],activation=train_parameters['activ_type'],gaussian_random_initialization=True)\n",
    "model.to(device)\n",
    "# OPTIMIZER (Adam)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=train_parameters['lrate'], weight_decay=train_parameters['l2_reg'])\n",
    "# lrscheduler\n",
    "#model.set_LRScheduler(opt,min_lr=5e-5)\n",
    "model.set_optimizer(opt)\n",
    "if valid_loader is not None:\n",
    "    # EarlyStopping\n",
    "    model.set_earlystopping(patience=train_parameters['es_patience'],\n",
    "                            min_delta=0.005,consecutive=train_parameters['es_consecutive'], save_best_model=True, log=False) \n",
    "# TRAIN\n",
    "model.fit(train_loader=train_loader,valid_loader=valid_loader,\n",
    "    standardize_inputs=train_parameters['standardize_inputs'],\n",
    "    standardize_outputs=train_parameters['standardize_outputs'],\n",
    "    loss_type=train_parameters['loss_type'],\n",
    "    n_eig=train_parameters['n_eig'],\n",
    "    nepochs=train_parameters['num_epochs'],\n",
    "    info=False, log_every=train_parameters['log_every'])\n",
    "#-- move the model back to cpu for convenience --#\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cvs #\n",
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]   \n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv1\",cmap=\"Set1\",ax=ax[0])\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv2\",cmap=\"Set1\",ax=ax[1])\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "c = ax[0].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c = ax[1].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c.clabel()\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(r\"$\\phi$\")\n",
    "ax[0].set_ylabel(r\"$\\psi$\")\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(r\"$\\phi$\")\n",
    "ax[1].set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tprime\n",
    "logweight = data[\"opes.bias\"].to_numpy()-max(data[\"opes.bias\"].to_numpy())\n",
    "logweight /= np.abs(min(logweight))\n",
    "logweight *= sim_parameters[\"beta\"]\n",
    "dt = t[1]-t[0]\n",
    "tprime = dt * np.cumsum(np.exp(logweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "# create time lagged dataset with different lag times\n",
    "for lag in lags:\n",
    "    #random split\n",
    "    # TensorDataset (x_t,x_lag,w_t,w_lag)\n",
    "    dataset = create_time_lagged_dataset(X,t=t,lag_time=np.round(lag,3),tprime=tprime,logweights=logweight,interval=[0,n_train+n_valid])\n",
    "    train_data, valid_data = random_split(dataset,[n_train,n_valid])\n",
    "    train_datasets.append(train_data)\n",
    "    valid_datasets.append(valid_data)\n",
    "\n",
    "train_loader = FastTensorDataLoader(train_datasets, batch_size=n_train,shuffle=shuffle)\n",
    "valid_loader = FastTensorDataLoader(valid_datasets, batch_size=n_valid,shuffle=shuffle)\n",
    "\n",
    "#-- TRAIN --#\n",
    "# MODEL\n",
    "model = DeepTICA_CV(train_parameters['nodes'],activation=train_parameters['activ_type'],gaussian_random_initialization=True)\n",
    "model.to(device)\n",
    "# OPTIMIZER (Adam)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=train_parameters['lrate'], weight_decay=train_parameters['l2_reg'])\n",
    "# lrscheduler\n",
    "#model.set_LRScheduler(opt,min_lr=5e-5)\n",
    "model.set_optimizer(opt)\n",
    "if valid_loader is not None:\n",
    "    # EarlyStopping\n",
    "    model.set_earlystopping(patience=train_parameters['es_patience'],\n",
    "                            min_delta=0.005,consecutive=train_parameters['es_consecutive'], save_best_model=True, log=False) \n",
    "# TRAIN\n",
    "model.fit(train_loader=train_loader,valid_loader=valid_loader,\n",
    "    standardize_inputs=train_parameters['standardize_inputs'],\n",
    "    standardize_outputs=train_parameters['standardize_outputs'],\n",
    "    loss_type=train_parameters['loss_type'],\n",
    "    n_eig=train_parameters['n_eig'],\n",
    "    nepochs=train_parameters['num_epochs'],\n",
    "    info=False, log_every=train_parameters['log_every'])\n",
    "#-- move the model back to cpu for convenience --#\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_lossfunction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cvs #\n",
    "data[\"cv1\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[0]\n",
    "data[\"cv2\"] = np.transpose(model(torch.Tensor(X)).detach().cpu().numpy())[1]   \n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv1\",cmap=\"Set1\",ax=ax[0])\n",
    "data.plot.hexbin(y=\"psi\",x=\"phi\",C=\"cv2\",cmap=\"Set1\",ax=ax[1])\n",
    "\n",
    "fes = np.loadtxt(\"angles/fes.txt\",delimiter=\" \")\n",
    "grid0 = np.loadtxt(\"angles/grid0.txt\",delimiter=\" \")\n",
    "grid1 = np.loadtxt(\"angles/grid1.txt\",delimiter=\" \")\n",
    "bounds = np.arange(0, 60, 5.)\n",
    "c = ax[0].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c = ax[1].contour(grid0, grid1, fes, bounds, linewidths=3,cmap=\"gray\",linestyles=\"dashed\",\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, ncolors=len(bounds)-1, clip=False), label=\"FES [Kj/mol]\",\n",
    ")\n",
    "c.clabel()\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(r\"$\\phi$\")\n",
    "ax[0].set_ylabel(r\"$\\psi$\")\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(r\"$\\phi$\")\n",
    "ax[1].set_ylabel(r\"$\\psi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Delta F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- each step is 1ps --#\n",
    "data = load_dataframe(folder+\"COLVAR\").filter(regex=\"^phi\").to_numpy()[:]\n",
    "logweight= np.transpose( load_dataframe(folder+\"COLVAR\").filter(regex=\"^opes.bias$\").to_numpy() )[0][:]\n",
    "logweight= (logweight-np.max(logweight))*sim_parameters[\"beta\"]\n",
    "s = data[:]\n",
    "weight = np.exp(logweight[:])\n",
    "fes,grid,bounds,error = compute_fes(s, weights=weight,\n",
    "                                    temp=sim_parameters[\"temp\"],\n",
    "                                    kbt=sim_parameters[\"kbt\"],\n",
    "                                    blocks=2,\n",
    "                                    bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                    plot=False)\n",
    "ind1 = (grid<0) \n",
    "ind2 = (grid>0) \n",
    "grid1 = grid[ ind1 ]\n",
    "grid2 = grid[ ind2 ] \n",
    "I1 = integrate.trapz(np.exp(-fes[ind1]*sim_parameters[\"beta\"]), grid1)\n",
    "I2 = integrate.trapz(np.exp(-fes[ind2]*sim_parameters[\"beta\"]), grid2)\n",
    "    \n",
    "res = (1/sim_parameters[\"beta\"])*np.log(I1/I2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- each step is 1ps --#\n",
    "#-- we are interested in the first 10 ns --#\n",
    "last = 1*1000*10 #last ns\n",
    "data = load_dataframe(folder+\"COLVAR\").filter(regex=\"^phi\").to_numpy()[:last]\n",
    "logweight= np.transpose( load_dataframe(folder+\"COLVAR\").filter(regex=\"^opes.bias$\").to_numpy() )[0][:last]\n",
    "logweight= (logweight-np.max(logweight))*sim_parameters[\"beta\"]\n",
    "#-- with CLEAR set to 1000 I perform the estimation every ns --#\n",
    "CLEAR=100\n",
    "\n",
    "deltaf = np.empty(0)\n",
    "for el in np.arange(CLEAR,len(data)+CLEAR,CLEAR):\n",
    "    s = data[:el]\n",
    "    weight = np.exp(logweight[:el])\n",
    "    fes,grid,bounds,error = compute_fes(s, weights=weight,\n",
    "                                        temp=sim_parameters[\"temp\"],\n",
    "                                        kbt=sim_parameters[\"kbt\"],\n",
    "                                        blocks=2,\n",
    "                                        bandwidth=sim_parameters[\"bandwidth\"],scale_by='range',\n",
    "                                        plot=False)\n",
    "    ind1 = (grid<0) \n",
    "    ind2 = (grid>0) \n",
    "    grid1 = grid[ ind1 ]\n",
    "    grid2 = grid[ ind2 ] \n",
    "    I1 = integrate.trapz(np.exp(-fes[ind1]*sim_parameters[\"beta\"]), grid1)\n",
    "    I2 = integrate.trapz(np.exp(-fes[ind2]*sim_parameters[\"beta\"]), grid2)\n",
    "    \n",
    "    deltaf = np.append(deltaf,(1/sim_parameters[\"beta\"])*np.log(I1/I2))\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot(np.arange(len(deltaf)),deltaf,label=\"Estimate\")\n",
    "res = np.full(len(deltaf),res)\n",
    "err = np.full(len(deltaf),0.2*kb*sim_parameters[\"temp\"])\n",
    "ax.plot(np.arange(len(deltaf)),res,linestyle='--',linewidth=3,color=\"g\",label=\"ref\")\n",
    "ax.fill_between(np.arange(len(deltaf)) , res-err, res+err , color=\"r\",zorder=0,alpha=0.3)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$t$ [ps$\\times$\"+str(CLEAR)+\"]\")\n",
    "ax.set_ylabel(r\"$\\Delta F$ [Kj/mol]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
