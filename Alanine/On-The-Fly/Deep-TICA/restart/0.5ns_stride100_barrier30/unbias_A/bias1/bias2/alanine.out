No protocol specified
              :-) GROMACS - gmx mdrun, 2020.6-plumed-2.9.0-dev (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf      Artem Zhmurov   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2020.6-plumed-2.9.0-dev
Executable:   /home/npedrani@iit.local/programs/gromacs-2020.6/install/bin/gmx_mpi
Data prefix:  /home/npedrani@iit.local/programs/gromacs-2020.6/install
Working dir:  /home/npedrani@iit.local/Desktop/Phd_main_Projects/Hyper-TICA/Alanine/On-The-Fly/Deep-TICA/restart/0.5ns_stride100_barrier30/unbias_A/bias1/bias2
Command line:
  gmx_mpi mdrun -s input.sA.tpr -cpo state.cpt -cpi state -noappend -deffnm alanine -plumed plumed.dat -ntomp 2 -nsteps 250000 -pin on -pinoffset 0 -pinstride 1

+++ Loading the PLUMED kernel runtime +++
+++ PLUMED_KERNEL="/home/npedrani@iit.local/programs/plumed2-pytorch/src/lib/libplumedKernel.so" +++
+++ Loading the PLUMED kernel runtime +++
+++ PLUMED_KERNEL="/home/npedrani@iit.local/programs/plumed2-pytorch/src/lib/libplumedKernel.so" +++
No protocol specified
Reading file input.sA.tpr, VERSION 2020.6-MODIFIED (single precision)
Overriding nsteps with value passed on the command line: 250000 steps, 500 ps
Changing nstlist from 10 to 50, rlist from 1.034 to 1.219


Using 1 MPI process
Using 2 OpenMP threads 


Overriding thread affinity set outside gmx mdrun
starting mdrun 'Generated by trjconv : Alanine in vacuum in water t=   0.00000'
750000 steps,   1500.0 ps (continuing from step 500000,   1000.0 ps).

-------------------------------------------------------
Program:     gmx mdrun, version 2020.6-plumed-2.9.0-dev

Standard library runtime error (possible bug):
(exception type: St13runtime_error)
The following operation failed in the TorchScript interpreter.
Traceback of TorchScript, serialized code (most recent call last):
  File "code/__torch__/mlcvs/tica/deep_tica.py", line 23, in forward
    _4 = self.nn
    _5 = self.RangeIn
    input = torch.div(torch.sub(x, self.MeanIn, alpha=1), _5)
                      ~~~~~~~~~ <--- HERE
    _6 = torch.sub((_4).forward(input, ), _3, alpha=1)
    H = torch.div(_6, _2)

Traceback of TorchScript, original code (most recent call last):
/home/npedrani@iit.local/programs/mlcvs/mlcvs/models/utils.py(65): normalize
/home/npedrani@iit.local/programs/mlcvs/mlcvs/models/nn.py(164): forward_nn
/home/npedrani@iit.local/programs/mlcvs/mlcvs/models/nn.py(187): forward
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/torch/nn/modules/module.py(860):
_slow_forward
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/torch/nn/modules/module.py(887):
_call_impl
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/torch/jit/_trace.py(940):
trace_module
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/torch/jit/_trace.py(742):
trace
/home/npedrani@iit.local/programs/mlcvs/mlcvs/models/nn.py(486): export
<ipython-input-49-9e8b4604bdae>(12): <module>
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3417):
run_code
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3337):
run_ast_nodes
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3146):
run_cell_async
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/async_helpers.py(68):
_pseudo_sync_runner
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2922):
_run_cell
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2877):
run_cell
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/zmqshell.py(536):
run_cell
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/ipkernel.py(306):
do_execute
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/gen.py(209):
wrapper
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/kernelbase.py(545):
execute_request
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/gen.py(209):
wrapper
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/kernelbase.py(268):
dispatch_shell
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/gen.py(209):
wrapper
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/kernelbase.py(365):
process_one
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/gen.py(748):
run
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/gen.py(787):
inner
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/ioloop.py(743):
_run_callback
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/ioloop.py(690):
<lambda>
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/asyncio/events.py(88):
_run
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/asyncio/base_events.py(1786):
_run_once
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/asyncio/base_events.py(541):
run_forever
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/tornado/platform/asyncio.py(149):
start
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel/kernelapp.py(612):
start
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/traitlets/config/application.py(845):
launch_instance
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/site-packages/ipykernel_launcher.py(16):
<module>
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/runpy.py(85):
_run_code
/home/npedrani@iit.local/anaconda3/envs/mlcvs/lib/python3.7/runpy.py(193):
_run_module_as_main
RuntimeError: The size of tensor a (45) must match the size of tensor b (44)
at non-singleton dimension 1

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
PLUMED instances was not properly deallocated in your code: 1
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[1153,1],0]
  Exit code:    1
--------------------------------------------------------------------------
No protocol specified
              :-) GROMACS - gmx mdrun, 2020.6-plumed-2.9.0-dev (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf      Artem Zhmurov   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2020.6-plumed-2.9.0-dev
Executable:   /home/npedrani@iit.local/programs/gromacs-2020.6/install/bin/gmx_mpi
Data prefix:  /home/npedrani@iit.local/programs/gromacs-2020.6/install
Working dir:  /home/npedrani@iit.local/Desktop/Phd_main_Projects/Hyper-TICA/Alanine/On-The-Fly/Deep-TICA/restart/0.5ns_stride100_barrier30/unbias_A/bias1/bias2
Command line:
  gmx_mpi mdrun -s input.sA.tpr -cpo state.cpt -cpi state -noappend -deffnm alanine -plumed plumed.dat -ntomp 2 -nsteps 250000 -pin on -pinoffset 0 -pinstride 1

+++ Loading the PLUMED kernel runtime +++
+++ PLUMED_KERNEL="/home/npedrani@iit.local/programs/plumed2-pytorch/src/lib/libplumedKernel.so" +++
+++ Loading the PLUMED kernel runtime +++
+++ PLUMED_KERNEL="/home/npedrani@iit.local/programs/plumed2-pytorch/src/lib/libplumedKernel.so" +++

Back Off! I just backed up alanine.part0003.log to ./#alanine.part0003.log.1#
No protocol specified
Reading file input.sA.tpr, VERSION 2020.6-MODIFIED (single precision)
Overriding nsteps with value passed on the command line: 250000 steps, 500 ps
Changing nstlist from 10 to 50, rlist from 1.034 to 1.219


Using 1 MPI process
Using 2 OpenMP threads 


Overriding thread affinity set outside gmx mdrun

Back Off! I just backed up alanine.part0003.xtc to ./#alanine.part0003.xtc.1#

Back Off! I just backed up alanine.part0003.edr to ./#alanine.part0003.edr.1#
starting mdrun 'Generated by trjconv : Alanine in vacuum in water t=   0.00000'
750000 steps,   1500.0 ps (continuing from step 500000,   1000.0 ps).

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:      265.450      132.725      200.0
                 (ns/day)    (hour/ns)
Performance:      325.486        0.074

GROMACS reminds you: "Always code as if the person who ends up maintaining your code is a violent psychopath who knows where you live." (Martin Golding)

